{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n",
    "<h3 align=\"left\"><font color='#DEB887'>Info</font></h3>\n",
    "    \n",
    "In this notebook the model for the Recommender System is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from typing import Dict, Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"Joropo_Expreriment\"\n",
    "\n",
    "if not mlflow.get_experiment_by_name(experiment_name):\n",
    "    mlflow.create_experiment(name=experiment_name) \n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"../data/processed/Final_data_for_ML.parquet\"\n",
    "path_model = '../models'\n",
    "epochs = 1000\n",
    "embedding_dimension = 64\n",
    "learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lowest_value(array):\n",
    "  lowest_value = array[0]\n",
    "  for value in array:\n",
    "    if value < lowest_value:\n",
    "      lowest_value = value\n",
    "  return lowest_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path_data)\n",
    "df = df[:1000]\n",
    "total_ratings= len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tf.data.Dataset.from_tensor_slices( {\"reviewerID\":tf.cast( df.reviewerID.values, tf.string),\n",
    "                                                \"title\" : tf.cast( df.title.values, tf.string)} )                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.map(lambda x: {\n",
    "    \"title\": x[\"title\"],\n",
    "    \"reviewerID\": x[\"reviewerID\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = ratings.map(lambda x: x[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "train = shuffled.take( int(total_ratings*0.8) )\n",
    "test = shuffled.skip(int(total_ratings*0.8)).take(int(total_ratings*0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_titles = products.batch(1_000)\n",
    "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"reviewerID\"])\n",
    "\n",
    "unique_product_titles = np.unique(np.concatenate(list(product_titles)))\n",
    "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_title_lookup = tf.keras.layers.StringLookup()\n",
    "product_title_lookup.adapt(ratings.map(lambda x: x[\"title\"]))\n",
    "product_title_embedding = tf.keras.layers.Embedding(input_dim=product_title_lookup.vocabulary_size(),output_dim=64)\n",
    "product_model = tf.keras.Sequential([product_title_lookup, product_title_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_lookup = tf.keras.layers.StringLookup()\n",
    "user_id_lookup.adapt(ratings.map(lambda x: x[\"reviewerID\"]))\n",
    "user_id_embedding = tf.keras.layers.Embedding(user_id_lookup.vocabulary_size(), 64)\n",
    "user_model = tf.keras.Sequential([user_id_lookup, user_id_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = tfrs.metrics.FactorizedTopK(\n",
    "  candidates=products.batch(128).map(user_model)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(\n",
    "  #metrics=metrics\n",
    "  metrics=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensModel(tfrs.Model):\n",
    "\n",
    "  def __init__(self, user_model, movie_model):\n",
    "    super().__init__()\n",
    "    self.product_model: tf.keras.Model = product_model\n",
    "    self.user_model: tf.keras.Model = user_model\n",
    "    self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    user_embeddings = self.user_model(features[\"reviewerID\"])\n",
    "    positive_movie_embeddings = self.product_model(features[\"title\"])\n",
    "\n",
    "    return self.task(user_embeddings, positive_movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel(user_model, product_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_call_back = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3, verbose=1, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 795ms/step - loss: 5348.0010 - regularization_loss: 0.0000e+00 - total_loss: 5348.0010\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5238.6758 - regularization_loss: 0.0000e+00 - total_loss: 5238.6758\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3532.4556 - regularization_loss: 0.0000e+00 - total_loss: 3532.4556\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1980.4767 - regularization_loss: 0.0000e+00 - total_loss: 1980.4767\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1849.8171 - regularization_loss: 0.0000e+00 - total_loss: 1849.8171\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1868.7206 - regularization_loss: 0.0000e+00 - total_loss: 1868.7206\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1844.0553 - regularization_loss: 0.0000e+00 - total_loss: 1844.0553\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1839.2410 - regularization_loss: 0.0000e+00 - total_loss: 1839.2410\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1844.3473 - regularization_loss: 0.0000e+00 - total_loss: 1844.3473\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1845.1138 - regularization_loss: 0.0000e+00 - total_loss: 1845.1138\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1837.6945 - regularization_loss: 0.0000e+00 - total_loss: 1837.6945\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1838.5800 - regularization_loss: 0.0000e+00 - total_loss: 1838.5800\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1839.1788 - regularization_loss: 0.0000e+00 - total_loss: 1839.1788\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1839.8975 - regularization_loss: 0.0000e+00 - total_loss: 1839.8975\n",
      "Epoch 14: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_hist = model.fit(cached_train, \n",
    "                        epochs=epochs, \n",
    "                        callbacks=[Loss_call_back])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 246ms/step - loss: 1070.5693 - regularization_loss: 0.0000e+00 - total_loss: 1070.5693\n"
     ]
    }
   ],
   "source": [
    "evaluate_history = model.evaluate(cached_test, \n",
    "                return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log params and metrics in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.start_run(experiment_id=experiment.experiment_id)\n",
    "\n",
    "mlflow.log_param(\"epochs\", epochs)\n",
    "mlflow.log_param(\"embedding_dimension\", embedding_dimension)\n",
    "mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "\n",
    "mlflow.log_metric(\"Loss\", find_lowest_value(model_hist.history['loss']))\n",
    "mlflow.log_metric(\"y_Loss\", evaluate_history[\"loss\"])\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 42: [b'Fast &amp; Fresh Baby Food Cookbook: 120 Ridiculously Simple and Naturally Wholesome Baby Food Recipes'\n",
      " b'Baking for Two: The Small-Batch Baking Cookbook for Sweet and Savory Treats'\n",
      " b'Baking for Two: The Small-Batch Baking Cookbook for Sweet and Savory Treats']\n"
     ]
    }
   ],
   "source": [
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "\n",
    "index.index_from_dataset(tf.data.Dataset.zip((products.batch(100), products.batch(100).map(model.product_model))))\n",
    "\n",
    "_, titles = index(tf.constant([\"42\"]))\n",
    "\n",
    "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations: [b'Fast &amp; Fresh Baby Food Cookbook: 120 Ridiculously Simple and Naturally Wholesome Baby Food Recipes'\n",
      " b'Baking for Two: The Small-Batch Baking Cookbook for Sweet and Savory Treats'\n",
      " b'Baking for Two: The Small-Batch Baking Cookbook for Sweet and Savory Treats']\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(index, path_model)\n",
    "loaded = tf.saved_model.load(path_model)\n",
    "scores, titles = loaded([\"42\"])\n",
    "\n",
    "print(f\"Recommendations: {titles[0][:3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Factored_Datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
